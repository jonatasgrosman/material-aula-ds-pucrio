{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in ./venv/lib/python3.9/site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.9/site-packages (2.13.1)\n",
      "Requirement already satisfied: evaluate in ./venv/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (1.24.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.9/site-packages (from transformers[torch]) (0.3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: accelerate>=0.20.2; extra == \"torch\" in ./venv/lib/python3.9/site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9; extra == \"torch\" in ./venv/lib/python3.9/site-packages (from transformers[torch]) (2.0.0+cu118)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./venv/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./venv/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: responses<0.19 in ./venv/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venv/lib/python3.9/site-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->transformers[torch]) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.9/site-packages (from accelerate>=0.20.2; extra == \"torch\"->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.9/site-packages (from torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./venv/lib/python3.9/site-packages (from torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.9/site-packages (from torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.9/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (3.25.0)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.9/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (15.0.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.9/site-packages (from sympy->torch!=1.12.0,>=1.9; extra == \"torch\"->transformers[torch]) (1.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/home/jonatas/project/material-aula-ds-pucrio/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# instalando dependências\n",
    "! pip install transformers[torch] datasets evaluate scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonatas/project/material-aula-ds-pucrio/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset yelp_review_full (/home/jonatas/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:00<00:00, 124.56it/s]\n",
      "Found cached dataset csv (/home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 2/2 [00:00<00:00, 971.24it/s]\n",
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-ad4c46bdb608265a.arrow\n",
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-5d4579aac5b7f1dc.arrow\n",
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-c9c34acae2286f14.arrow\n",
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-4a6751b5d61783c9.arrow\n",
      "Loading cached shuffled indices for dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-a57d6ffca794cc38.arrow\n",
      "Loading cached shuffled indices for dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-a57d6ffca794cc38.arrow\n",
      "Loading cached shuffled indices for dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-53c9118c9b3b039b.arrow\n"
     ]
    }
   ],
   "source": [
    "# baixando o dataset\n",
    "\n",
    "from datasets import Features, Value, ClassLabel, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# custom\n",
    "\n",
    "# class_names = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "# features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})\n",
    "custom_dataset = load_dataset('csv', data_files={'train': 'train_small.csv', 'test': 'test_small.csv'}, \n",
    "                              column_names=['label', 'text'], skiprows=[0])\n",
    "                            #   column_names=['label', 'text'], skiprows=[0], features=features)\n",
    "custom_dataset = custom_dataset.class_encode_column(column=\"label\")\n",
    "\n",
    "# criando os splits\n",
    "\n",
    "small_train_dataset = custom_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_dev_dataset = custom_dataset[\"train\"].shuffle(seed=42).select(range(1000,1050))\n",
    "small_test_dataset = custom_dataset[\"test\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 4274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 3920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"I got food poisoning after eating at this location, I almost went to the hospital after about 12hrs of vomiting and diarrhea. I will NEVER eat Arby's again\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': 'I have been going to Jimmy Johns Sandwich shop for 2 years now.I absolutely love there sandwiches and trust me Im a big fan. But today when I came in I was very disappointed. When I received my #9 I had ask the nice guy at the counter for 4 out of the hundereds that were in the giant box of packets of yellow mustard. He said he was only able to give me 2, 2 packets of yellow mustard for the giant sandwich I had in front of me was a joke. He said he would have to charge me 25 cents for a 5 cent packet of mustard. Thats like going to Mcdonalds and ordering 2 large frys and getting two packets of ketchup to fulfill your needs. When I asked Derek why would he have to charge a absurd price he stated \\\\\"Because Jimmy doesnt want you to ruin your sandwich\\\\\". Iam from the greatest country on Earth the United States of America,If I want to I should have the right to put what ever I please on a sandwich. I understand that Derek was doing his job and doing what he was told, so I asked for the \\\\\"head\\\\\" manager he directed me to Maranda(mustard natzi) a short overweight bitter woman, you could tell she was picked on in school by the look on her face and it was her mission to get back at the world. Anyways she was sitting there while all this was happening and she heard everything that was said. She would not give me the time of day, all she said was \\\\\"company policey\\\\\" then  continued to do what ever it was she was doing. This is not right. I had to buy some candy from a local 7-11 just to get my mustard packets. This was the worst experice Ive ever had at one of there locations.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(small_train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# carregando o modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# carregar o bert com pesos aleatórios a partir do config\n",
    "bert_config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_config.num_labels = 5\n",
    "zero_model = AutoModelForSequenceClassification.from_config(bert_config)\n",
    "\n",
    "# carregando o tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-7f3c53c4e270cc3f.arrow\n",
      "Loading cached processed dataset at /home/jonatas/.cache/huggingface/datasets/csv/default-7dcc470f94fe408b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-43c3db75cfad482e.arrow\n",
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "# tokenizando os dados\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_small_dev_dataset = small_dev_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_small_test_dataset = small_test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': 'I have been going to Jimmy Johns Sandwich shop for 2 years now.I absolutely love there sandwiches and trust me Im a big fan. But today when I came in I was very disappointed. When I received my #9 I had ask the nice guy at the counter for 4 out of the hundereds that were in the giant box of packets of yellow mustard. He said he was only able to give me 2, 2 packets of yellow mustard for the giant sandwich I had in front of me was a joke. He said he would have to charge me 25 cents for a 5 cent packet of mustard. Thats like going to Mcdonalds and ordering 2 large frys and getting two packets of ketchup to fulfill your needs. When I asked Derek why would he have to charge a absurd price he stated \\\\\"Because Jimmy doesnt want you to ruin your sandwich\\\\\". Iam from the greatest country on Earth the United States of America,If I want to I should have the right to put what ever I please on a sandwich. I understand that Derek was doing his job and doing what he was told, so I asked for the \\\\\"head\\\\\" manager he directed me to Maranda(mustard natzi) a short overweight bitter woman, you could tell she was picked on in school by the look on her face and it was her mission to get back at the world. Anyways she was sitting there while all this was happening and she heard everything that was said. She would not give me the time of day, all she said was \\\\\"company policey\\\\\" then  continued to do what ever it was she was doing. This is not right. I had to buy some candy from a local 7-11 just to get my mustard packets. This was the worst experice Ive ever had at one of there locations.',\n",
       " 'input_ids': [101,\n",
       "  146,\n",
       "  1138,\n",
       "  1151,\n",
       "  1280,\n",
       "  1106,\n",
       "  4479,\n",
       "  11673,\n",
       "  16377,\n",
       "  9242,\n",
       "  4130,\n",
       "  1111,\n",
       "  123,\n",
       "  1201,\n",
       "  1208,\n",
       "  119,\n",
       "  146,\n",
       "  7284,\n",
       "  1567,\n",
       "  1175,\n",
       "  25326,\n",
       "  1105,\n",
       "  3496,\n",
       "  1143,\n",
       "  146,\n",
       "  1306,\n",
       "  170,\n",
       "  1992,\n",
       "  5442,\n",
       "  119,\n",
       "  1252,\n",
       "  2052,\n",
       "  1165,\n",
       "  146,\n",
       "  1338,\n",
       "  1107,\n",
       "  146,\n",
       "  1108,\n",
       "  1304,\n",
       "  9333,\n",
       "  119,\n",
       "  1332,\n",
       "  146,\n",
       "  1460,\n",
       "  1139,\n",
       "  108,\n",
       "  130,\n",
       "  146,\n",
       "  1125,\n",
       "  2367,\n",
       "  1103,\n",
       "  3505,\n",
       "  2564,\n",
       "  1120,\n",
       "  1103,\n",
       "  4073,\n",
       "  1111,\n",
       "  125,\n",
       "  1149,\n",
       "  1104,\n",
       "  1103,\n",
       "  177,\n",
       "  6775,\n",
       "  5686,\n",
       "  1116,\n",
       "  1115,\n",
       "  1127,\n",
       "  1107,\n",
       "  1103,\n",
       "  4994,\n",
       "  2884,\n",
       "  1104,\n",
       "  27284,\n",
       "  1104,\n",
       "  3431,\n",
       "  1538,\n",
       "  2881,\n",
       "  119,\n",
       "  1124,\n",
       "  1163,\n",
       "  1119,\n",
       "  1108,\n",
       "  1178,\n",
       "  1682,\n",
       "  1106,\n",
       "  1660,\n",
       "  1143,\n",
       "  123,\n",
       "  117,\n",
       "  123,\n",
       "  27284,\n",
       "  1104,\n",
       "  3431,\n",
       "  1538,\n",
       "  2881,\n",
       "  1111,\n",
       "  1103,\n",
       "  4994,\n",
       "  14327,\n",
       "  146,\n",
       "  1125,\n",
       "  1107,\n",
       "  1524,\n",
       "  1104,\n",
       "  1143,\n",
       "  1108,\n",
       "  170,\n",
       "  8155,\n",
       "  119,\n",
       "  1124,\n",
       "  1163,\n",
       "  1119,\n",
       "  1156,\n",
       "  1138,\n",
       "  1106,\n",
       "  2965,\n",
       "  1143,\n",
       "  1512,\n",
       "  18748,\n",
       "  1111,\n",
       "  170,\n",
       "  126,\n",
       "  9848,\n",
       "  17745,\n",
       "  1104,\n",
       "  1538,\n",
       "  2881,\n",
       "  119,\n",
       "  1337,\n",
       "  1116,\n",
       "  1176,\n",
       "  1280,\n",
       "  1106,\n",
       "  150,\n",
       "  1665,\n",
       "  3842,\n",
       "  18728,\n",
       "  1116,\n",
       "  1105,\n",
       "  13649,\n",
       "  123,\n",
       "  1415,\n",
       "  175,\n",
       "  1616,\n",
       "  1116,\n",
       "  1105,\n",
       "  2033,\n",
       "  1160,\n",
       "  27284,\n",
       "  1104,\n",
       "  180,\n",
       "  2105,\n",
       "  17143,\n",
       "  1643,\n",
       "  1106,\n",
       "  14414,\n",
       "  1240,\n",
       "  2993,\n",
       "  119,\n",
       "  1332,\n",
       "  146,\n",
       "  1455,\n",
       "  6749,\n",
       "  1725,\n",
       "  1156,\n",
       "  1119,\n",
       "  1138,\n",
       "  1106,\n",
       "  2965,\n",
       "  170,\n",
       "  21321,\n",
       "  3945,\n",
       "  1119,\n",
       "  2202,\n",
       "  165,\n",
       "  107,\n",
       "  2279,\n",
       "  4479,\n",
       "  2144,\n",
       "  1204,\n",
       "  1328,\n",
       "  1128,\n",
       "  1106,\n",
       "  10677,\n",
       "  1240,\n",
       "  14327,\n",
       "  165,\n",
       "  107,\n",
       "  119,\n",
       "  146,\n",
       "  2312,\n",
       "  1121,\n",
       "  1103,\n",
       "  4459,\n",
       "  1583,\n",
       "  1113,\n",
       "  2746,\n",
       "  1103,\n",
       "  1244,\n",
       "  1311,\n",
       "  1104,\n",
       "  1738,\n",
       "  117,\n",
       "  1409,\n",
       "  146,\n",
       "  1328,\n",
       "  1106,\n",
       "  146,\n",
       "  1431,\n",
       "  1138,\n",
       "  1103,\n",
       "  1268,\n",
       "  1106,\n",
       "  1508,\n",
       "  1184,\n",
       "  1518,\n",
       "  146,\n",
       "  4268,\n",
       "  1113,\n",
       "  170,\n",
       "  14327,\n",
       "  119,\n",
       "  146,\n",
       "  2437,\n",
       "  1115,\n",
       "  6749,\n",
       "  1108,\n",
       "  1833,\n",
       "  1117,\n",
       "  2261,\n",
       "  1105,\n",
       "  1833,\n",
       "  1184,\n",
       "  1119,\n",
       "  1108,\n",
       "  1500,\n",
       "  117,\n",
       "  1177,\n",
       "  146,\n",
       "  1455,\n",
       "  1111,\n",
       "  1103,\n",
       "  165,\n",
       "  107,\n",
       "  1246,\n",
       "  165,\n",
       "  107,\n",
       "  2618,\n",
       "  1119,\n",
       "  2002,\n",
       "  1143,\n",
       "  1106,\n",
       "  14973,\n",
       "  7807,\n",
       "  113,\n",
       "  1538,\n",
       "  2881,\n",
       "  9468,\n",
       "  5745,\n",
       "  1182,\n",
       "  114,\n",
       "  170,\n",
       "  1603,\n",
       "  1166,\n",
       "  7150,\n",
       "  9178,\n",
       "  1590,\n",
       "  117,\n",
       "  1128,\n",
       "  1180,\n",
       "  1587,\n",
       "  1131,\n",
       "  1108,\n",
       "  3015,\n",
       "  1113,\n",
       "  1107,\n",
       "  1278,\n",
       "  1118,\n",
       "  1103,\n",
       "  1440,\n",
       "  1113,\n",
       "  1123,\n",
       "  1339,\n",
       "  1105,\n",
       "  1122,\n",
       "  1108,\n",
       "  1123,\n",
       "  2862,\n",
       "  1106,\n",
       "  1243,\n",
       "  1171,\n",
       "  1120,\n",
       "  1103,\n",
       "  1362,\n",
       "  119,\n",
       "  10756,\n",
       "  1116,\n",
       "  1131,\n",
       "  1108,\n",
       "  2807,\n",
       "  1175,\n",
       "  1229,\n",
       "  1155,\n",
       "  1142,\n",
       "  1108,\n",
       "  5664,\n",
       "  1105,\n",
       "  1131,\n",
       "  1767,\n",
       "  1917,\n",
       "  1115,\n",
       "  1108,\n",
       "  1163,\n",
       "  119,\n",
       "  1153,\n",
       "  1156,\n",
       "  1136,\n",
       "  1660,\n",
       "  1143,\n",
       "  1103,\n",
       "  1159,\n",
       "  1104,\n",
       "  1285,\n",
       "  117,\n",
       "  1155,\n",
       "  1131,\n",
       "  1163,\n",
       "  1108,\n",
       "  165,\n",
       "  107,\n",
       "  1419,\n",
       "  2021,\n",
       "  1183,\n",
       "  165,\n",
       "  107,\n",
       "  1173,\n",
       "  1598,\n",
       "  1106,\n",
       "  1202,\n",
       "  1184,\n",
       "  1518,\n",
       "  1122,\n",
       "  1108,\n",
       "  1131,\n",
       "  1108,\n",
       "  1833,\n",
       "  119,\n",
       "  1188,\n",
       "  1110,\n",
       "  1136,\n",
       "  1268,\n",
       "  119,\n",
       "  146,\n",
       "  1125,\n",
       "  1106,\n",
       "  4417,\n",
       "  1199,\n",
       "  13962,\n",
       "  1121,\n",
       "  170,\n",
       "  1469,\n",
       "  128,\n",
       "  118,\n",
       "  1429,\n",
       "  1198,\n",
       "  1106,\n",
       "  1243,\n",
       "  1139,\n",
       "  1538,\n",
       "  2881,\n",
       "  27284,\n",
       "  119,\n",
       "  1188,\n",
       "  1108,\n",
       "  1103,\n",
       "  4997,\n",
       "  4252,\n",
       "  3365,\n",
       "  4396,\n",
       "  146,\n",
       "  2707,\n",
       "  1518,\n",
       "  1125,\n",
       "  1120,\n",
       "  1141,\n",
       "  1104,\n",
       "  1175,\n",
       "  4541,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_small_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "MODEL_PATH = \"test_trainer_v2\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    max_steps=1000,\n",
    "    learning_rate=3e-5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # model=model,\n",
    "    model=zero_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_small_train_dataset,\n",
    "    eval_dataset=tokenized_small_dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonatas/project/material-aula-ds-pucrio/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7717, 'learning_rate': 2.97e-05, 'epoch': 0.31}\n",
      "{'loss': 1.6349, 'learning_rate': 2.94e-05, 'epoch': 0.62}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_log = pd.DataFrame(trainer.state.log_history)\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.plot(y=[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log[\"eval_loss\"].dropna().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log[\"eval_accuracy\"].dropna().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliando o modelo\n",
    "\n",
    "trainer.evaluate(tokenized_small_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo e rodando um predict\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "text = \"This is a really good restaurant\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model(**encoded_input)\n",
    "output.logits\n",
    "\n",
    "# convertendo a saída para probabilidades\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probabilities = F.softmax(output.logits, dim=-1)\n",
    "probabilities\n",
    "\n",
    "# convertendo a saída para labels\n",
    "\n",
    "predicted_class_idx = torch.argmax(probabilities, dim=-1)\n",
    "predicted_class_idx\n",
    "\n",
    "probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de como carregar um dataset customizado\n",
    "\n",
    "custom_dataset = load_dataset('csv', data_files={'train': 'train_small.csv', 'test': 'test_small.csv'})\n",
    "custom_dataset\n",
    "\n",
    "# ClassLabel(names = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
